# Quantum Computing in Short
A personal summary after reading what seems like a pretty good ACM paper on the topic

I have looked for a while for an approachable description of quantum computing and quantum algorithms that does not need a graduate degree in the related areas to understand. Someplace flagged [this](https://dl.acm.org/doi/10.1145/3517340) December 2022 ACM preview around 2022/07/20. It is by far the most detail rich description, that I can still pretty much follow. So, I wanted to type out my mental model and notes while reading it to both remind myself later and to see if it helps others to understand the topic in a math-light way.

First, what is a quantum computer in general? (for more detail on any part of this, see the paper. It also points to yet more detail if desired) The part that is covered everywhere is the qbit, which the given implementation has some number of. The paper mostly uses two, real world IBM examples. One has five qbits and the other has 16. Qbits could have more than two measured states depending on which quantum property you are working with, but most, and all in the paper, use two (0,1). I am going to skip the entanglement part except to mention that the computers lose their needed entanglement over time. This means that if your program/circuit takes too long, it will not work. Also, the chance that you got the right answer is better, the shorter your program is. A given quantum computer will implement some “gates” natively in hardware. If you need other gates or actions, these have to be built from the native gates. The computer will do some of these for you, a compiler could do some for you or you may have to do some of that translation yourself. After you have a circuit/program ready, you will need to run it a number of times and look at the distribution of measured outputs. This is because a quantum computer does not guarantee the correct answer on any given run, only to trend toward the correct answer as you get to a large number of runs. Then there is also a noise profile to the quantum computer. This will say how fast each of the qbits on the system might decohere/un-entangle with the others. Finally, you probably can’t use just any qbit as the input or output of any type of gate. There is a connection graph for each quantum computer to tell you which ones can be used as input and output together for some gate types. So the summary of things to know when shopping around for your quantum computer would be:

* Number of qbits
* Time for a gate
* Gate types implemented, both native and supported
* Time until decoherence (un-entanglement)
* Qbit connections and graph

Moving from the quantum computer to quantum programming, the article gives example programs in a couple different ways; diagrams, circuits in IBM support software and as a Python plug-in. As they guide you through each, it seems like you need to setup your program as a set of linear algebra (matrix math) problems that fit the computer you are working with. Care is needed in the setup, the main operation to do the work, and then a way to read out your answer. To get a speed-up advantage, the main part takes advantage of the entanglement to solve a variable over a continuous range of values or add dimensions. The paper also gave some base paradigms that get used in a bunch of the currently implemented programs. These seem to represent the things that might offer a quantum advantage once the number of qbits get sufficiently large and the noise gets sufficiently small. The Grover Operator seems to lead to a number of search related items including a few I had not heard of before, like finding a triangle or circuit in a graph. I assume that the Quantum Fourier Transform would be used anywhere a regular one would be used. Harrow-Hassidim-Lloyd program would provide a scaler solution to a system of linear equations. The wiki page for this lists least squares fit as one of the applications it could be used for and also machine learning training. The other mentioned paradigms seem to be mainly used for simulating quantum systems. Shor’s algorithm was not in the paradigm list, but is the other ready-to-go program which would find factors and could break certain types of encryption.

It seems that there are a limited number of out-of-the-box programs that could be used at the moment. Also, the estimates for something like using Shor’s algorithm to break current encryption are in the many millions of qbits needed. Currently, state of the art systems seem to all have <200. I assume that things like machine learning training would need to have the data to be trained on all represented as qbits, in which case that would need many billions of qbits for a fair sized system. Not impossible if you get semiconductor sized quantum CPUs. The paper also highlighted some difficulties with trying some of the programs:

* The program takes too many steps and simulator gets the right answer, but the real quantum computer ends up with so much noise, it didn’t work at all. This happened in the paper on multiple programs.
* Systems with larger numbers of qbits have longer gate execution time and can run into the decoherence problem after fewer steps.
* Sometimes you can run more machine-shots to get over noise and converge to an answer and sometimes not. In an episode of Data Skeptic, there was a guest covering a quantum version of the K-means algorithm and they seemed to run into this in practice, finding that they needed log(N) instead of constant time. It seemed to be due to this issue.
* Some of the quantum algorithms rely on evaluation functions that are as bad or worse than the classical algorithms they replace. Maybe efficient ones can be found for the quantum systems, but that would have to happen before the containing program would be of any use.
* Some quantum work has inspired new classical algorithms. The paper points this out as part of their Harrow-Hassidim-Lloyd program discussion. This is not a quantum computer issue, but increases the improvements needed to a quantum computer in order to compete with a classical one.
* Similar to the point above, sometimes, like with the quantum search algorithm, an odd assumption is made, like an unordered database. If I want to find something fast in a database, I would probably index that key, which would give me constant look up times. It seems like sometimes the example given would be done differently in the real world case.
* The last algorithm the paper tries to implement is Quantum Error Correction. It has very limited success with this, which would probably kill the use of a system that does useful things. I expect this would end up custom built into the system and other papers specifically on the topic seem to claim progress. It would be great to see a follow on in a few years to where this issue is at.

Which things look like they will be usable in the future? If the number of qbits doubles every year for 20 years and the noise can be kept low and the times until decoherence reasonable:

* Someone will break today’s encryption. > 10 years, <= 20. The factoring example also worked (for 15) on the real quantum computer. However, a special for that computer, algorithm was used to make it operational on the five qbits. This problem gets a lot of attention, so there is also more information about it.

Ok, just one. It feels like there will be some cool items solved with maximizing or minimizing. To be cool, the problem would have to be something like very exact numbers of component items being way better than solutions that are near-by. I am thinking something like a super strong alloy that only gets that way when very specific numbers of elemental atoms are mixed. I don’t know if that is a valid example, but something where just making 10 million guesses, which we can do easily today, does not get you reasonably close to a good outcome. Of course, you would need a fitting, linear algebra way to model the thing you are trying to maximize and I don’t know of any. I hope we run into a couple in the next 20 years.

UPDATE: The just one comment above is unfair. DWave has current customers that seem to target some optimization problems, so that must also be a tractable problem that should see better use moving forward